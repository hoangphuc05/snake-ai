Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, sigmoid, sigmoid, sigmoid
[[16,sigmoid], [32,sigmoid] , [64,sigmoid], [16,sigmoid]]
Average Score: 18.6 | Average Accuracy: 0.9419233095645905
Output as file: sigm-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, relu, relu, relu
[[16,relu], [32,relu] , [64,relu], [16,relu]]
Average Score: 40.2 | Average Accuracy: 0.9946932236353556
Output as file: relu-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: elu, elu, elu, elu
[[16,elu], [32,elu] , [64,elu], [16,elu]]
Average Score: 20.4 | Average Accuracy: 0.9940279444058736
Output as file: elu-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, sigmoid, relu, sigmoid
[[16,relu], [32,sigmoid] , [64,relu], [16,sigmoid]]
Average Score: 36.0 | Average Accuracy: 0.9884296830495198
Output as file: rsrs-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, relu, sigmoid, sigmoid
[[16,relu], [32,relu] , [64,sigmoid], [16,sigmoid]]
Average Score: 29.4 | Average Accuracy: 0.9909705221652985
Output as file: rrss-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, relu, relu, sigmoid
[[16,sigmoid], [32,relu] , [64,relu], [16,sigmoid]]
Average Score: 31.6 | Average Accuracy: 0.9830060362815857
Output as file: srrs-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, sigmoid, relu, relu
[[16,sigmoid], [32,sigmoid] , [64,relu], [16,relu]]
Average Score: 35.6 | Average Accuracy: 0.991387062072754
Output as file: ssrr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: elu, sigmoid, elu, sigmoid
[[16,elu], [32,sigmoid] , [64,elu], [16,sigmoid]]
Average Score: 27.4 | Average Accuracy: 0.9885206266244253
Output as file: eses-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: elu, elu, sigmoid, sigmoid
[[16,elu], [32,elu] , [64,sigmoid], [16,sigmoid]]
Average Score: 28.6 | Average Accuracy: 0.9830080819129944
Output as file: eess-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, elu, elu, sigmoid
[[16,sigmoid], [32,elu] , [64,elu], [16,sigmoid]]
Average Score: 26.6 | Average Accuracy: 0.9874706407388051
Output as file: sees-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, sigmoid, elu, elu
[[16,sigmoid], [32,sigmoid] , [64,elu], [16,elu]]
Average Score: 33.6 | Average Accuracy: 0.9845298612117768
Output as file: ssee-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, elu, relu, elu
[[16,relu], [32,elu] , [64,relu], [16,elu]]
Average Score: 24.8 | Average Accuracy: 0.9949201369285583
Output as file: rere-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, relu, elu, elu
[[16,relu], [32,relu] , [64,elu], [16,elu]]
Average Score: 44.0 | Average Accuracy: 0.9962641108036041
Output as file: rree-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: elu, relu, relu, elu
[[16,elu], [32,relu] , [64,relu], [16,elu]]
Average Score: 39.2 | Average Accuracy: 0.9952382250626882
Output as file: erre-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: elu, elu, relu, relu
[[16,elu], [32,elu] , [64,relu], [16,relu]]
Average Score: 30.4 | Average Accuracy: 0.9949787978331248
Output as file: eerr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, relu, sigmoid, sigmoid
[[16,sigmoid], [32,relu] , [64,sigmoid], [16,sigmoid]]
Average Score: 21.8 | Average Accuracy: 0.988189126253128
Output as file: srss-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, sigmoid, relu, sigmoid
[[16,sigmoid], [32,sigmoid] , [64,relu], [16,sigmoid]]
Average Score: 25.0 | Average Accuracy: 0.9781187538305919
Output as file: ssrs-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, sigmoid, sigmoid, relu
[[16,sigmoid], [32,sigmoid] , [64,sigmoid], [16,relu]]
Average Score: 33.4 | Average Accuracy: 0.9822836860020956
Output as file: sssr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, sigmoid, sigmoid, sigmoid
[[16,relu], [32,sigmoid] , [64,sigmoid], [16,sigmoid]]
Average Score: 34.0 | Average Accuracy: 0.9842476936181387
Output as file: rsss-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, elu, sigmoid, sigmoid
[[16,sigmoid], [32,elu] , [64,sigmoid], [16,sigmoid]]
Average Score: 29.2 | Average Accuracy: 0.980837398370107
Output as file: sess-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, sigmoid, elu, sigmoid
[[16,sigmoid], [32,sigmoid] , [64,elu], [16,sigmoid]]
Average Score: 39.4 | Average Accuracy: 0.9865952746073405
Output as file: sses-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, sigmoid, sigmoid, elu
[[16,sigmoid], [32,sigmoid] , [64,sigmoid], [16,elu]]
Average Score: 31.2 | Average Accuracy: 0.9783331644535065
Output as file: ssse-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: elu, sigmoid, sigmoid, sigmoid
[[16,elu], [32,sigmoid] , [64,sigmoid], [16,sigmoid]]
Average Score: 43.0 | Average Accuracy: 0.9805070308844248
Output as file: esss-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, elu, relu, relu
[[16,relu], [32,elu] , [64,relu], [16,relu]]
Average Score: 20.8 | Average Accuracy: 0.9951095366477967
Output as file: rerr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, relu, elu, relu
[[16,relu], [32,relu] , [64,elu], [16,relu]]
Average Score: 41.2 | Average Accuracy: 0.9946518437067667
Output as file: rrer-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, relu, relu, elu
[[16,relu], [32,relu] , [64,relu], [16,elu]]
Average Score: 36.0 | Average Accuracy: 0.9929597647984822
Output as file: rrre-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: elu, relu, relu, relu
[[16,elu], [32,relu] , [64,relu], [16,relu]]
Average Score: 42.0 | Average Accuracy: 0.9960985859235127
Output as file: errr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, sigmoid, relu, relu
[[16,relu], [32,sigmoid] , [64,relu], [16,relu]]
Average Score: 30.0 | Average Accuracy: 0.9814667534828186
Output as file: rsrr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, relu, sigmoid, relu
[[16,relu], [32,relu] , [64,sigmoid], [16,relu]]
Average Score: 33.6 | Average Accuracy: 0.9868199137846628
Output as file: rrsr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: relu, relu, relu, sigmoid
[[16,relu], [32,relu] , [64,relu], [16,sigmoid]]
Average Score: 30.8 | Average Accuracy: 0.9931421180566152
Output as file: rrrs-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 4 layers w/ following layers and activation functions: 
Layers: 16 32 64 16
Activation: sigmoid, relu, relu, relu
[[16,sigmoid], [32,relu] , [64,relu], [16,relu]]
Average Score: 25.0 | Average Accuracy: 0.9875368042786916
Output as file: srrr-16-32-64-16-bot2.h5
-------------------------------------------------------------------- 
 
-------------------------------------------------------------------- 
| 2 layer start w/ sigmoid, relu, elu, softmax                     |
-------------------------------------------------------------------- 

-------------------------------------------------------------------- 

Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: sigmoid, sigmoid
[[8,sigmoid], [16,sigmoid]]
Average Score: 49.4 | Average Accuracy: 0.9744824526707331
Output as file: ss-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: relu, relu
[[8,relu], [16,relu]]
Average Score: 26.6 | Average Accuracy: 0.9815124543507894
Output as file: rr-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: elu, elu
[[8,elu], [16,elu]]
Average Score: 32.4 | Average Accuracy: 0.9922042238712311
Output as file: ee-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: softmax, softmax
[[8,softmax], [16,softmax]]
Average Score: 19.4 | Average Accuracy: 0.9426147345701853
Output as file: smxsmx-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: sigmoid, relu
[[8,sigmoid], [16,relu]]
Average Score: 32.6 | Average Accuracy: 0.9861905558904012
Output as file: sr-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: relu, sigmoid
[[8,relu], [16,sigmoid]]
Average Score: 48.4 | Average Accuracy: 0.986347668170929
Output as file: rs-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: sigmoid, elu
[[8,sigmoid], [16,elu]]
Average Score: 28.2 | Average Accuracy: 0.9882370972633362
Output as file: se-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: elu, sigmoid
[[8,elu], [16,sigmoid]]
Average Score: 7.8 | Average Accuracy: 0.9742257523536683
Output as file: es-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: sigmoid, softmax
[[8,sigmoid], [16,softmax]]
Average Score: 35.4 | Average Accuracy: 0.9548657941818237
Output as file: ssmx-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: softmax, sigmoid
[[8,softmax], [16,sigmoid]]
Average Score: 18.4 | Average Accuracy: 0.9766344932715098
Output as file: smxs-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: relu, elu
[[8,relu], [16,elu]]
Average Score: 35.4 | Average Accuracy: 0.9868960809707642
Output as file: re-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: elu, relu
[[8,elu], [16,relu]]
Average Score: 42.0 | Average Accuracy: 0.9934185945987701
Output as file: er-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: softmax, relu
[[8,softmax], [16,relu]]
Average Score: 25.4 | Average Accuracy: 0.9837006465593974
Output as file: smxr-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: relu, softmax
[[8,relu], [16,softmax]]
Average Score: 14.2 | Average Accuracy: 0.9694746669133504
Output as file: rsmx-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: softmax, elu
[[8,softmax], [16,elu]]
Average Score: 26.2 | Average Accuracy: 0.979619386990865
Output as file: smxe-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 8 16
Activation: elu, softmax
[[8,elu], [16,softmax]]
Average Score: 27.8 | Average Accuracy: 0.9670704738299052
Output as file: esmx-8-16-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: sigmoid, sigmoid
[[16,sigmoid], [8,sigmoid]]
Average Score: 35.0 | Average Accuracy: 0.9778261307875316
Output as file: ss-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: relu, relu
[[16,relu], [8,relu]]
Average Score: 34.2 | Average Accuracy: 0.9867517026265462
Output as file: rr-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: elu, elu
[[16,elu], [8,elu]]
Average Score: 29.4 | Average Accuracy: 0.9890056081612905
Output as file: ee-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: softmax, softmax
[[16,softmax], [8,softmax]]
Average Score: 29.4 | Average Accuracy: 0.9512610977888107
Output as file: smxsmx-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: sigmoid, relu
[[16,sigmoid], [8,relu]]
Average Score: 39.2 | Average Accuracy: 0.9920807611942292
Output as file: sr-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: relu, sigmoid
[[16,relu], [8,sigmoid]]
Average Score: 31.0 | Average Accuracy: 0.9913415861129761
Output as file: rs-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: sigmoid, elu
[[16,sigmoid], [8,elu]]
Average Score: 37.4 | Average Accuracy: 0.9867105484008789
Output as file: se-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: elu, sigmoid
[[16,elu], [8,sigmoid]]
Average Score: 33.0 | Average Accuracy: 0.9882111811637878
Output as file: es-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: sigmoid, softmax
[[16,sigmoid], [8,softmax]]
Average Score: 27.8 | Average Accuracy: 0.9694842143853506
Output as file: ssmx-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: softmax, sigmoid
[[16,softmax], [8,sigmoid]]
Average Score: 38.8 | Average Accuracy: 0.9800572967529297
Output as file: smxs-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: relu, elu
[[16,relu], [8,elu]]
Average Score: 47.2 | Average Accuracy: 0.9944849566618601
Output as file: re-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: elu, relu
[[16,elu], [8,relu]]
Average Score: 34.4 | Average Accuracy: 0.9794459025065104
Output as file: er-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: softmax, relu
[[16,softmax], [8,relu]]
Average Score: 49.8 | Average Accuracy: 0.981306004524231
Output as file: smxr-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: relu, softmax
[[16,relu], [8,softmax]]
Average Score: 25.2 | Average Accuracy: 0.9816982146104177
Output as file: rsmx-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: softmax, elu
[[16,softmax], [8,elu]]
Average Score: 32.8 | Average Accuracy: 0.9821256637573242
Output as file: smxe-16-8-bot2.h5
-------------------------------------------------------------------- 
 
Running Analysis for 2 layers w/ following layers and activation functions: 
Layers: 16 8
Activation: elu, softmax
[[16,elu], [8,softmax]]
Average Score: 32.8 | Average Accuracy: 0.9879553890228272
Output as file: esmx-16-8-bot2.h5
-------------------------------------------------------------------- 
 
